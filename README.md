# ğŸ§© MONet-Replication â€“ Unsupervised Scene Decomposition

This repository is a **clean PyTorch reimplementation** of  
**MONet: Unsupervised Scene Decomposition and Representation (Burgess et al., 2019)**.

The goal is to turn the original paperâ€™s **architecture, math, and block diagram** into a readable and modular codebase.

- Recursive **attention-based object discovery** ğŸª  
- **Component-wise VAEs** for object modeling ğŸ§¬  
- Full **ELBO objective** for generative training ğŸ§   

**Paper reference:** [Unsupervised Scene Decomposition and Representation](https://arxiv.org/abs/1901.11390) ğŸ“„

---

## ğŸŒ  Overview â€“ How MONet Works

MONet decomposes a scene into objects **one by one** using recursive attention.  
Each object is modeled with its own VAE and the final image is composed from all parts.
```text
Input Image x (B, 3, H, W)
        â¬‡ï¸
CNN Encoder (feature maps)
        â¬‡ï¸
Attention Net Î±Ïˆ(x, scope)
        â¬‡ï¸
Recurrent Attention
  - Generates masks m_k
  - Updates scope
        â¬‡ï¸
Component-wise VAE (one per mask m_k)
  - Encoder: qÏ†(z_k | x, m_k)
  - Decoder: pÎ¸(x | z_k)
        â¬‡ï¸
Mask Decoder pÎ¸(c | {z_k})
  - Predicts masks from latent slots
        â¬‡ï¸
Compositor
  - Soft-masked summation: xÌ‚ = Î£_k m_k * x_k
        â¬‡ï¸
Output:
  - xÌ‚       â† Reconstructed image
  - masks   â† Attention masks
  - z_slots â† Latent vectors
  - mus, logvars â† Latent stats

```
---

## ğŸ§® Core Math

### Recursive Attention
```math
m_k = s_k Â· Ïƒ(Î±_Ïˆ(x, s_k))  
s_{k+1} = s_k Â· (1 âˆ’ m_k)
```

### Component-wise VAE
```math
q(z_k | x, m_k) = N(Î¼_k, Ïƒ_kÂ²)  
p(z_k) = N(0, I)
```

### Scene Reconstruction
```math
xÌ‚ = Î£_k m_k Â· x_k
```

### ELBO Objective
```math
L = reconstruction + Î² Â· KL(z) + Î³ Â· KL(masks)
```

---

## ğŸ§  What This Model Does

- Decomposes scenes into **K object slots**  
- Learns **unsupervised object masks**  
- Trains with a **full probabilistic generative model**  
- Produces object-level latent representations  

This is MONet exactly as described in the paper â€” just turned into PyTorch.

---

## ğŸ“¦ Repository Structure

```bash
MONet-Replication/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ encoder/
â”‚   â”‚   â”œâ”€â”€ cnn_encoder.py         # Image â†’ feature map
â”‚   â”‚   â””â”€â”€ mask_encoder.py        # (Image, Mask) â†’ latent posterior params
â”‚   â”‚
â”‚   â”œâ”€â”€ attention/
â”‚   â”‚   â”œâ”€â”€ attention_net.py       # Î±Ïˆ(x, scope) â†’ attention logits
â”‚   â”‚   â”œâ”€â”€ scope_update.py        # Recursive scope logic
â”‚   â”‚   â”œâ”€â”€ mask_generator.py      # mk generation step-by-step
â”‚   â”‚   â””â”€â”€ recurrent_attention.py # Full MONet attention loop
â”‚   â”‚
â”‚   â”œâ”€â”€ vae/
â”‚   â”‚   â”œâ”€â”€ encoder.py             # qÏ†(z_k | x, m_k)
â”‚   â”‚   â”œâ”€â”€ decoder.py             # pÎ¸(x | z_k)
â”‚   â”‚   â”œâ”€â”€ mask_decoder.py        # pÎ¸(c | {z_k})
â”‚   â”‚   â””â”€â”€ component_vae.py       # One masked VAE forward
â”‚   â”‚
â”‚   â”œâ”€â”€ decoder/
â”‚   â”‚   â””â”€â”€ compositor.py          # Î£_k m_k * x_k
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ monet.py               # Full MONet forward pipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ loss/
â”‚   â”‚ 	â””â”€â”€ monet_loss.py   		 # Full MONet ELBO
â”‚   â”‚
â”‚   â””â”€â”€ config.py                   # slots, latent_dim, image_size
â”‚
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---


## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
