# ğŸ’ MONet-Replication â€“ Modular Object-Centric Learning

This repository provides a **PyTorch-based replication** of  
**MONet: Unsupervised Scene Decomposition and Representation**.

The goal is to **understand and implement object-centric representation learning**  
through recurrent attention and component-wise VAEs â€” not to chase SOTA numbers.

- Decomposes scenes into **interpretable object slots** ğŸ§¿  
- Uses **recurrent attention** to segment objects sequentially ğŸŒ€  
- Learns **disentangled latent representations** per object ğŸ§¬  
- Fully modular and easy to plug into vision research pipelines âš™ï¸  

**Paper reference:** [MONet â€“ Burgess et al., 2019](https://arxiv.org/abs/1901.11390) ğŸ“„

---

## ğŸŒŒ Overview â€“ MONet Pipeline

![MONet Overview](images/figmix.jpg)

MONet decomposes an image into a set of object-centric latent variables by iteratively attending to different regions of the scene.

Core idea:

> Sequentially attend to different parts of the image, encode each region with a VAE, and reconstruct the scene by softly composing object reconstructions.

High-level process:

1. Extract image features using a CNN encoder.
2. Generate an attention mask using a recurrent attention network.
3. Encode the masked region into a latent variable.
4. Decode each latent into an object reconstruction.
5. Soft-compose all object reconstructions into the final image.

---

## ğŸ” Model Structure

The model consists of two main components:

### 1. Recurrent Attention Network

Generates a sequence of soft masks that decompose the image into object regions.

At step $k$:

$$
m_k = \alpha_\psi(x, s_{k-1})
$$

Where:
- $m_k$ is the attention mask
- $\alpha_\psi$ is the attention network
- $s_{k-1}$ is the remaining unexplained scope

The scope is updated as:

$$
s_k = s_{k-1} \cdot (1 - m_k)
$$


### 2. Component-wise VAE

Each mask is used to encode and decode a single object:

Encoder:

$$
q_\phi(z_k | x, m_k)
$$

Decoder:

$$
p_\theta(x_k | z_k)
$$

Reconstruction is performed via soft composition:

$$
\hat{x} = \sum_k m_k \cdot x_k
$$

---

## ğŸ§® Training Objective â€“ MONet ELBO

The full model is trained by maximizing the Evidence Lower Bound:

```math
\mathcal{L} =
\sum_k \mathbb{E}_{q_\phi(z_k \mid x, m_k)}[\log p_\theta(x \mid z_k, m_k)]
- \beta \, \mathrm{KL}(q_\phi(z_k \mid x, m_k) \Vert p(z_k))
+ \lambda \, \mathcal{L}_{mask}
```

Where:
- Reconstruction likelihood is Gaussian
- Prior $p(z)$ is standard normal
- Mask loss enforces partitioning consistency
- $\beta$ controls disentanglement strength

---

## ğŸ§  What the Model Learns

- Object-centric latent slots instead of global embeddings  
- Unsupervised segmentation through attention  
- Disentangled representations per object  
- Scene decomposition without labels  

This makes MONet a foundation model for:
- Object-based reasoning  
- Compositional generalization  
- Interpretable vision systems  

---

## ğŸ“¦ Repository Structure

```bash
MONet-Replication/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ encoder/
â”‚   â”‚   â”œâ”€â”€ cnn_encoder.py         # Image â†’ feature map
â”‚   â”‚   â””â”€â”€ mask_encoder.py        # (Image, Mask) â†’ latent posterior params
â”‚   â”‚
â”‚   â”œâ”€â”€ attention/
â”‚   â”‚   â”œâ”€â”€ attention_net.py       # Î±Ïˆ(x, scope) â†’ attention logits
â”‚   â”‚   â”œâ”€â”€ scope_update.py        # Recursive scope logic
â”‚   â”‚   â”œâ”€â”€ mask_generator.py      # mk generation step-by-step
â”‚   â”‚   â””â”€â”€ recurrent_attention.py # Full MONet attention loop
â”‚   â”‚
â”‚   â”œâ”€â”€ vae/
â”‚   â”‚   â”œâ”€â”€ encoder.py             # qÏ†(z_k | x, m_k)
â”‚   â”‚   â”œâ”€â”€ decoder.py             # pÎ¸(x | z_k)
â”‚   â”‚   â”œâ”€â”€ mask_decoder.py        # pÎ¸(c | {z_k})
â”‚   â”‚   â””â”€â”€ component_vae.py       # One masked VAE forward
â”‚   â”‚
â”‚   â”œâ”€â”€ decoder/
â”‚   â”‚   â””â”€â”€ compositor.py          # Î£_k m_k * x_k
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ monet.py               # Full MONet forward pipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ loss/
â”‚   â”‚ 	â””â”€â”€ monet_loss.py   		 # Full MONet ELBO
â”‚   â”‚
â”‚   â””â”€â”€ config.py                   # slots, latent_dim, image_size
â”‚
â”‚
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ figmix.jpg               # MONet overview figure
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---


## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
